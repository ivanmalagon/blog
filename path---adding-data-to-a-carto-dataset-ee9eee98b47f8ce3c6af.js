webpackJsonp([0xdbb72d72c55a],{503:function(e,t){e.exports={data:{site:{siteMetadata:{title:"Ivan Malagon Blog",author:"Ivan Malagon"}},markdownRemark:{id:"/Users/ivan/Code/Personal/blog/src/pages/adding-data-to-a-carto-dataset/index.md absPath of file >>> MarkdownRemark",html:'<h2>Eating our own dog food</h2>\n<p>In CARTO we have a Slack channel called <code>#using-carto</code> where <a href="https://twitter.com/inigo_medina">Iñigo</a> sets a weekly challenge. We have to use CARTO tools to answer a very open question. Last week it was:</p>\n<div class="gatsby-highlight">\n      <pre class="language-none"><code class="language-none">I am visiting Madrid and a friend has given me her BiciMAD card.\nI love maps and above all maps made by myself. :)\nI wonder how I could create a map that allowed me  to see easily\nMadrid neighbourhoods and the BiciMAD parkings located in them.\nIt would be great to select parkings with the greater capacity,\nso I could be fairly sure of finding a free place for my bike.</code></pre>\n      </div>\n<p>It’s fun to see how we all end up answering the same question with different approaches. We have to document every step we take, pointing at the pains we discover along the way.</p>\n<p>A common question here is <em>How can I insert data in CARTO to an existing dataset?</em>. My take this week with <code>#using-carto</code> is answering not the original challenge but this other question.</p>\n<p>BiciMAD is the public bike service of Madrid. They provide an API to check the status of its station network. What I want to do is to feed a dataset in CARTO with the results of an API. It’s a small dataset: 172 bike stations with 11 fields each, mostly numbers. It looks like a job for the SQL API!</p>\n<h2>Updating your dataset via SQL API</h2>\n<p>As stated <a href="https://carto.com/docs/carto-engine/sql-api/">in the docs</a>, CARTO SQL API allows you to interact with your tables and data inside CARTO as if you were running SQL statements against a normal database. It even has batch queries to send long-running jobs to the database.</p>\n<p>Using the BiciMAD bike stations info as an example, I’ll show three different approaches to update your dataset. In all of them we’ll have the restriction of using batch queries. They have a limitation of 16KB size, so we’ll take that into account as we proceed.</p>\n<h3>Replace all your data</h3>\n<p>If we don’t need to save historical data, what we can do is to erase all data from the dataset and replace it with the result of the BiciMAD API call. This ensures to have our data up-to-date.</p>\n<p>As we are using batch queries what we should do is:</p>\n<ol>\n<li>\n<p>Sort the records coming from the API by their ID. We need to delete only the records that will go in the current job. Having them sorted will allow us to know exactly the range of records we need to erase first.</p>\n</li>\n<li>\n<p>Start writing the SELECT query.</p>\n</li>\n</ol>\n<div class="gatsby-highlight">\n      <pre class="language-none"><code class="language-none">INSERT INTO ${username}.${table} (id, name, dock_bikes, ... , the_geom) VALUES</code></pre>\n      </div>\n<ol start="3">\n<li>Append the values for each record until it reaches the maximum size for a job, saving the first and last id</li>\n</ol>\n<div class="gatsby-highlight">\n      <pre class="language-none"><code class="language-none">(1, \'Avda America\', 24, ... ), \n(2, \'Quevedo\', 15, ... ),\n(3, \'Suchil\', 7, ... ),</code></pre>\n      </div>\n<ol start="4">\n<li>Create a DELETE query with the ids of the records that will go within this job.</li>\n</ol>\n<div class="gatsby-highlight">\n      <pre class="language-none"><code class="language-none">DELETE FROM ${username}.${table} WHERE id >= 1 AND id <= 103</code></pre>\n      </div>\n<ol start="5">\n<li>Create the job, setting first the DELETE query and then the INSERT one.</li>\n</ol>\n<div class="gatsby-highlight">\n      <pre class="language-none"><code class="language-none">[\n  "DELETE FROM ...",\n  "INSERT INTO ...\n]</code></pre>\n      </div>\n<h4>PROS</h4>\n<ul>\n<li>Easy to implement.</li>\n<li>Inserting multiple rows in the same SELECT statement allows us to fit more work into a single job.</li>\n</ul>\n<h4>CONS</h4>\n<ul>\n<li>While the rows are being inserted, previous ones have already been deleted. It’s a fast operation but you can see only part of the data in the meantime.</li>\n</ul>\n<h3>Upsert</h3>\n<p>If you don’t feel comfortable with seeing only part of the data while it’s being updated, you can create UPSERT queries instead of a bulk delete and insert.</p>\n<div class="gatsby-highlight">\n      <pre class="language-none"><code class="language-none">  // You need a unique index on `id` in order to get a conflict\n\n  INSERT INTO ${username}.${table} (id, name, dock_bikes, ..., the_geom)\n    VALUES (${values}, ${geom})\n    ON CONFLICT (id) DO\n      UPDATE SET ${keysAndValues}, the_geom=${geom};\n  `;</code></pre>\n      </div>\n<h4>PROS</h4>\n<ul>\n<li>Data is always up to date. No visible glitchs while the jobs are running.</li>\n</ul>\n<h4>CONS</h4>\n<ul>\n<li>Larger queries. That means less queries per job and then, more jobs to run.</li>\n<li>What do we do with records that die? In our case, if a station gets removed, since we’re only inserting or updating, it’ll be kept in the dataset. Fixing this would require more complex logic to compare the current data with the new one.</li>\n</ul>\n<h3>Append new data</h3>\n<p>In this challenge it doesn’t make sense the two previous approaches. If someone is interested in knowing real-time the bike availability, she’d use the official app. We cannot keep the same pace.</p>\n<p>The use case that it’s more interesting is to save the historical data of the stations, so we can run analyses later to get insights about the hotest stations or usage patterns.</p>\n<p>And it’s even easier than the two previous strategies. We only need to add a timestamp column and then using a multiple rows statement.</p>\n<h4>PROS</h4>\n<ul>\n<li>Historical data.</li>\n<li>Easiest to implement.</li>\n</ul>\n<h4>CONS</h4>\n<ul>\n<li>Table becomes very big if you append very often. You’d need to update less frequently or implement a expiry policy.</li>\n</ul>\n<h2>Final implementation</h2>\n<p>I went for the third approach, updating the table every hour instead of aiming for ‘real-time’.</p>\n<p>I implemented a Google Cloud HTTP function that calls the API and then builds the batch query jobs. <a href="https://gist.github.com/ivanmalagon/492fc7a92e54118df77f7a469cd277bb">The code of the function is in this Gist</a>. Then, I use an uptime service that pings that URL every hour. I chose StatusCake but there are plenty of them. I did it like that for convenience but you can set a cron job from a server as well.</p>\n<p>The dataset is public and growing. You can access it <a href="https://team.carto.com/u/hacheka/tables/bicimad_inc/public">here</a>. I plan to let it getting fat all April and then use the resulting dataset to run some analyses on it. Feel free to use it!</p>',frontmatter:{title:"Adding data to a CARTO dataset",date:"April 01, 2018"}}},pathContext:{slug:"/adding-data-to-a-carto-dataset/",previous:{fields:{slug:"/visualizing-rhymes-structures/"},frontmatter:{title:"Visualizing rhyme structures"}},next:!1}}}});
//# sourceMappingURL=path---adding-data-to-a-carto-dataset-ee9eee98b47f8ce3c6af.js.map